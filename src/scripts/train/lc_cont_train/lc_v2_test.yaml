2025-02-11 16:46:19.194	Dustin-Schwenk-MacBook-Pro:0	olmo_core.utils:195	INFO	Setting env var 'OMP_NUM_THREADS' to '8'
2025-02-11 16:46:19.194	Dustin-Schwenk-MacBook-Pro:0	olmo_core.utils:195	INFO	Setting env var 'TOKENIZERS_PARALLELISM' to 'false'
LcContTrain(
    run_name='run01',
    launch=BeakerLaunchConfig(
        name='run01-437a21fc',
        cmd=['OLMo2-7B-lc_p1.py', 'train', 'run01', 'ai2/jupiter-cirrascale-2', '--launch.num_nodes=2', '--launch.workspace=ai2-long-contexts'],
        budget='ai2/oe-training',
        task_name='train',
        workspace='ai2-long-contexts',
        description=None,
        setup_steps=[
            'git clone "$REPO_URL" .',
            'git checkout "$GIT_REF"',
            'git submodule update --init --recursive',
            'conda shell.bash activate base',
            "pip install -e '.[all]'",
            'pip install --upgrade beaker-py',
            'pip freeze',
            'mkdir -p ~/.aws',
            'printenv AWS_CONFIG > ~/.aws/config',
            'printenv AWS_CREDENTIALS > ~/.aws/credentials'
        ],
        beaker_image='olmo-core-tch260cu124',
        num_nodes=2,
        num_gpus=8,
        clusters=['ai2/jupiter-cirrascale-2'],
        shared_filesystem=True,
        priority=<Priority.normal: 'normal'>,
        preemptible=True,
        retries=None,
        env_vars=[BeakerEnvVar(name='NCCL_DEBUG', value='WARN')],
        env_secrets=[
            BeakerEnvSecret(name='BEAKER_TOKEN', secret='dustins_BEAKER_TOKEN'),
            BeakerEnvSecret(name='WANDB_API_KEY', secret='dustins_WANDB_API_KEY'),
            BeakerEnvSecret(name='COMET_API_KEY', secret='dustins_COMET_API_KEY'),
            BeakerEnvSecret(name='AWS_CONFIG', secret='dustins_AWS_CONFIG'),
            BeakerEnvSecret(name='AWS_CREDENTIALS', secret='dustins_AWS_CREDENTIALS'),
            BeakerEnvSecret(name='R2_ENDPOINT_URL', secret='R2_ENDPOINT_URL'),
            BeakerEnvSecret(name='WEKA_ENDPOINT_URL', secret='WEKA_ENDPOINT_URL'),
            BeakerEnvSecret(name='SLACK_WEBHOOK_URL', secret='SLACK_WEBHOOK_URL')
        ],
        nfs=False,
        weka_buckets=[BeakerWekaBucket(bucket='oe-training-default', mount='/weka/oe-training-default')],
        allow_dirty=False,
        host_networking=None
    ),
    model=TransformerTrainModuleConfig(
        rank_microbatch_size=65536,
        max_sequence_length=65536,
        optim=AdamWConfig(
            group_overrides=[OptimGroupOverride(params=['embeddings.weight'], opts={'weight_decay': 0.0})],
            compile=False,
            lr=3e-05,
            betas=[0.9, 0.95],
            eps=1e-08,
            weight_decay=0.1,
            foreach=None,
            fused=True
        ),
        max_grad_norm=1.0,
        scheduler=CosWithWarmup(lr_field='lr', initial_lr_field='initial_lr', warmup_steps=2000, alpha_f=0.1, t_max=None, warmup_min_lr=0.0),
        compile_model=True,
        float8_config=Float8Config(
            scaling_type_input='dynamic',
            scaling_type_weight='dynamic',
            scaling_type_grad_output='dynamic',
            enable_fsdp_float8_all_gather=True,
            precompute_float8_dynamic_scale_for_fsdp=True,
            compile=None,
            enabled=False
        ),
        dp_config=TransformerDataParallelConfig(name='fsdp', param_dtype='bfloat16', reduce_dtype='float32', num_replicas=None, wrapping_strategy='fine_grained'),
        tp_config=TransformerTensorParallelConfig(degree=2, enable_async=False, loss_parallel=True),
        ep_config=None,
        ac_config=TransformerActivationCheckpointingConfig(mode='full', block_interval=None, modules=None),
        fused_loss=False,
        compile_loss=True,
        z_loss_multiplier=1e-05,
        state_dict_save_opts=None,
        state_dict_load_opts=None,
        load_key_mapping=None,
        autocast_precision=None,
        label_ignore_index=-100
    ),
    optim=AdamWConfig(
        group_overrides=[OptimGroupOverride(params=['embeddings.weight'], opts={'weight_decay': 0.0})],
        compile=False,
        lr=2e-05,
        betas=[0.9, 0.95],
        eps=1e-08,
        weight_decay=0.1,
        foreach=None,
        fused=True
    ),
    dataset=NumpyDatasetConfig(
        tokenizer=TokenizerConfig(vocab_size=100278, eos_token_id=100257, pad_token_id=100277, bos_token_id=None, identifier='allenai/dolma2-tokenizer'),
        name='fsl',
        source_mixture_config=None,
        sequence_length=65536,
        max_target_sequence_length=None,
        max_sequence_length=None,
        min_sequence_length=None,
        vsl_curriculum=None,
        paths=None,
        mix='dolmino',
        mix_base_dir='/weka/oe-training-default/ai2-llm',
        dtype=None,
        metadata=None,
        include_instance_metadata=True,
        generate_doc_lengths=False,
        expand_glob=False,
        work_dir='/weka/oe-training-default/ai2-llm/checkpoints/dustins/dataset-cache'
    ),
    data_loader=NumpyDataLoaderConfig(global_batch_size=16777216, seed=34521, work_dir=None, num_threads=None, num_workers=4, prefetch_factor=None, target_device_type=None),
    trainer=TrainerConfig(
        save_folder='gs://ai2-llm/checkpoints/dustins/run01',
        work_dir=None,
        load_path='s3://ai2-llm/checkpoints/dustins/long-contexts/OLMo-2-1124-7B-Instruct/',
        load_strategy='if_available',
        checkpointer=CheckpointerConfig(work_dir=None, save_overwrite=None, pre_download=False, save_thread_count=1, load_thread_count=32, throttle_uploads=True),
        device=None,
        save_overwrite=True,
        max_duration=Duration(value=20000000, unit='tokens'),
        cancel_check_interval=10,
        hard_stop=None,
        metrics_collect_interval=10,
        callbacks={
            'checkpointer': CheckpointerCallback(
                save_interval=1000,
                ephemeral_save_interval=None,
                pre_train_checkpoint=None,
                save_async=True,
                remove='ephemeral_only',
                _latest_checkpoint_step=-1,
                _latest_checkpoint_path='',
                _checkpoints=[],
                _ephemeral_checkpoints=[],
                _checkpoints_to_remove=[]
            ),
            'wandb': WandBCallback(
                enabled=True,
                name='run01',
                project='long-contexts',
                entity='ai2-llm',
                group=None,
                tags=None,
                notes=None,
                config=None,
                cancel_tags=['cancel', 'canceled', 'cancelled'],
                cancel_check_interval=10
            ),
            'gpu_monitor': GPUMemoryMonitorCallback(device_id=None, _num_alloc_retries=0),
            'config_saver': ConfigSaverCallback(config=None, fname='config.json'),
            'garbage_collector': GarbageCollectorCallback(gc_interval=1000, enabled=True, _start_state=None),
            'downstream_evaluator': DownstreamEvaluatorCallbackConfig(
                tasks=[
                    'mmlu_stem_mc_5shot',
                    'mmlu_humanities_mc_5shot',
                    'mmlu_social_sciences_mc_5shot',
                    'mmlu_other_mc_5shot',
                    'mmlu_stem_mc_5shot_test',
                    'mmlu_humanities_mc_5shot_test',
                    'mmlu_social_sciences_mc_5shot_test',
                    'mmlu_other_mc_5shot_test',
                    'arc_challenge_test_rc_5shot',
                    'arc_easy_test_rc_5shot',
                    'csqa_val_rc_5shot',
                    'hellaswag_val_rc_5shot',
                    'openbookqa_test_rc_5shot',
                    'piqa_val_rc_5shot',
                    'socialiqa_val_rc_5shot'
                ],
                tokenizer=TokenizerConfig(vocab_size=100278, eos_token_id=100257, pad_token_id=100277, bos_token_id=None, identifier='allenai/dolma2-tokenizer'),
                eval_interval=1000,
                eval_duration=Duration(value=1, unit='epochs'),
                log_interval=5,
                enabled=True
            )
        },
        async_bookkeeping=None
    ),
    load_path='s3://ai2-llm/checkpoints/dustins/long-contexts/OLMo-2-1124-7B-Instruct/',
    init_seed=12536
)
