import math
from typing import Any, Dict, List

import torch


def split_batch(batch: Dict[str, Any], microbatch_size: int) -> List[Dict[str, Any]]:
    """
    Split a batch (such as one generated by the :class:`DataCollator`) into a list of micro-batches.
    """
    batch_size = batch["input_ids"].shape[0]
    if batch_size <= microbatch_size:
        return [batch]
    else:
        micro_batches = {}
        for key, value in batch.items():
            if isinstance(value, torch.Tensor):
                micro_batches[key] = value.split(microbatch_size, dim=0)
            elif isinstance(value, list):
                micro_batches[key] = [
                    value[microbatch_size * i : microbatch_size * i + microbatch_size]
                    for i in range(math.ceil(batch_size / microbatch_size))
                ]
            else:
                raise RuntimeError(f"unexpected item in batch: '{key}={value}'")
        return [
            {key: value[i] for key, value in micro_batches.items()}
            for i in range(len(micro_batches["input_ids"]))
        ]


def melt_batch(batch: Dict[str, Any], target_sequence_length: int) -> Dict[str, Any]:
    """
    "Melts" a batch by shortening the sequence length and proportionally increasing the number
    of instances.
    """
    current_batch_size, current_sequence_length = batch["input_ids"].shape
    if current_sequence_length <= target_sequence_length:
        return batch

    if current_sequence_length % target_sequence_length != 0:
        raise RuntimeError(
            "current sequence of batch must be a multiple of the target sequence length "
            "in order to 'melt' the batch"
        )

    ratio = current_sequence_length // target_sequence_length

    new_batch: Dict[str, Any] = {}
    for key, value in batch.items():
        if isinstance(value, torch.Tensor):
            if value.shape == (current_batch_size, current_sequence_length):
                new_batch[key] = value.reshape(-1, target_sequence_length)
            elif value.shape == (current_batch_size,) or value.shape == (current_batch_size, 1):
                new_batch[key] = value.repeat_interleave(ratio)
            else:
                raise RuntimeError(
                    f"unable to melt '{key}' tensor in batch with shape '{value.shape}'"
                )
        elif isinstance(value, list) and len(value) > 0:
            new_batch[key] = []
            for item in value:
                if isinstance(item, list):
                    if len(item) != current_sequence_length:
                        raise RuntimeError(f"unexpected item length for '{key}' in batch")
                    for i in range(ratio):
                        new_batch[key].append(item[i * ratio : i * ratio + target_sequence_length])
                else:
                    for _ in range(ratio):
                        new_batch[key].append(item)
        else:
            raise RuntimeError(f"unexpected item in batch: '{key}={value}'")

    return new_batch


def truncate_batch(batch: Dict[str, Any], target_sequence_length: int) -> Dict[str, Any]:
    """
    Truncate the instances in a batch to ``target_sequence_length``.
    """
    current_batch_size, current_sequence_length = batch["input_ids"].shape
    if current_sequence_length <= target_sequence_length:
        return batch

    new_batch: Dict[str, Any] = {}
    for key, value in batch.items():
        if isinstance(value, torch.Tensor):
            if value.shape == (current_batch_size, current_sequence_length):
                new_batch[key] = value[:, :target_sequence_length]
            elif value.shape == (current_batch_size,) or value.shape == (current_batch_size, 1):
                new_batch[key] = value
            else:
                raise RuntimeError(
                    f"unable to truncate '{key}' tensor in batch with shape '{value.shape}'"
                )
        elif isinstance(value, list) and len(value) > 0:
            new_batch[key] = []
            for item in value:
                if isinstance(item, list):
                    if len(item) != current_sequence_length:
                        raise RuntimeError(f"unexpected item length for '{key}' in batch")
                    new_batch[key].append(item[:target_sequence_length])
                else:
                    new_batch[key].append(item)
        else:
            raise RuntimeError(f"unexpected item in batch: '{key}={value}'")

    return new_batch
