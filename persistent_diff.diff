diff --git a/src/olmo_core/nn/attention/__init__.py b/src/olmo_core/nn/attention/__init__.py
index 32dda15..c89b2c5 100644
--- a/src/olmo_core/nn/attention/__init__.py
+++ b/src/olmo_core/nn/attention/__init__.py
@@ -31,6 +31,7 @@ from .flash_attn_api import (
     dispatch_flash_attn_qkvpacked,
     dispatch_ring_flash_attn,
     dispatch_ring_flash_attn_qkvpacked,
+    dispatch_paddle_flash_attn,  # Add this import
 )
 from .ring import (
     RingAttentionLlama3LoadBalancer,
@@ -59,6 +60,8 @@ class SlidingWindowAttentionConfig(Config):
     window_size: int = 4096
     force_first: bool = True
     force_last: bool = True
+    use_paddle_flash: bool = False
+    num_global_tokens: int = 0
 
     def should_use_swa(self, layer_idx: int, n_layers: int) -> bool:
         if self.force_first:
@@ -113,6 +116,8 @@ class AttentionConfig(Config):
     use_flash: Optional[bool] = None
     dtype: DType = DType.float32
     sliding_window: Optional[SlidingWindowAttentionConfig] = None
+    num_global_tokens: int = 0
+    use_paddle_flash: bool = False
 
     def num_params(self, d_model: int) -> int:
         """
@@ -270,6 +275,8 @@ class Attention(AttentionBase):
         qk_norm: Optional[LayerNormConfig] = None,
         dropout: float = 0.0,
         use_flash: bool = False,
+        use_paddle_flash: bool = False,  # Add this
+        num_global_tokens: int = 0,      # Add this
         window_size: Optional[int] = None,
         dtype: torch.dtype = torch.float32,
         init_device: str = "cpu",
@@ -310,7 +317,9 @@ class Attention(AttentionBase):
             assert isinstance(rope_class, (RotaryEmbedding, ComplexRotaryEmbedding))
             self.rope = rope_class
 
-        self.use_flash = use_flash
+        self.use_flash = use_flash            
+        self.use_paddle_flash = use_paddle_flash  # Store this
+        self.num_global_tokens = num_global_tokens  # Store this
 
         # Translate window size so that we only look left, not right.
         if window_size is not None:
@@ -345,8 +354,23 @@ class Attention(AttentionBase):
         max_doc_len_k: Optional[int] = None,
         local_k_slice: Optional[slice] = None,
         scale: Optional[float] = None,
+        **kwargs
     ) -> torch.Tensor:
         att: torch.Tensor
+    
+        # Check if we should use PaddlePaddle flash attention
+        use_paddle = kwargs.get("use_paddle_flash", False)
+        if use_paddle:
+            from .flash_attn_api import dispatch_paddle_flash_attn
+            return dispatch_paddle_flash_attn(
+                q, k, v,
+                window_size=self.window_size[0] if isinstance(self.window_size, tuple) else self.window_size,
+                num_global_tokens=kwargs.get("num_global_tokens", 0),
+                dropout_p=self.dropout_p,
+                softmax_scale=scale,
+                causal=True
+            )
+            
         if self.cp_enabled:
             assert self._cp_pg is not None and self._cp_load_balancer is not None
             if not self.use_flash:
@@ -491,6 +515,7 @@ class Attention(AttentionBase):
             )
 
         # shape: (batch_size, seq_len, n_heads, head_dim)
+        # Inside the forward method of Attention class
         att = self.sdpa(
             q,
             k,
@@ -502,6 +527,8 @@ class Attention(AttentionBase):
             max_doc_len_q=max_doc_len_q,
             max_doc_len_k=max_doc_len_k,
             local_k_slice=local_k_slice,
+            use_paddle_flash=getattr(self, 'use_paddle_flash', False),  # Pass from instance variable
+            num_global_tokens=getattr(self, 'num_global_tokens', 0)     # Pass from instance variable
         )
 
         # shape: (batch_size, seq_len, d_model)
diff --git a/src/olmo_core/nn/attention/flash_attn_api.py b/src/olmo_core/nn/attention/flash_attn_api.py
index 65d544d..5707f6b 100644
--- a/src/olmo_core/nn/attention/flash_attn_api.py
+++ b/src/olmo_core/nn/attention/flash_attn_api.py
@@ -15,11 +15,76 @@ try:
 except ImportError:
     ring_flash_attn = None
 
-
 def _flatten_batch_dim(x: torch.Tensor) -> torch.Tensor:
     B, T, *other = x.shape
     return x.view(B * T, *other)
 
+def sliding_window_causal_idx(b, h, q_idx, kv_idx, sliding_window, persistent_tokens):
+    causal_mask = q_idx >= kv_idx
+    window_mask = q_idx - kv_idx <= sliding_window 
+    persistent_mask = kv_idx <= persistent_tokens
+    return causal_mask & (window_mask | persistent_mask)
+
+def dispatch_paddle_flash_attn(
+    q: torch.Tensor,
+    k: torch.Tensor,
+    v: torch.Tensor,
+    window_size: int = 128,
+    num_global_tokens: int = 0,
+    dropout_p: float = 0.0,
+    softmax_scale: Optional[float] = None,
+    causal: bool = True,
+    **kwargs
+) -> torch.Tensor:
+    """Dispatch to PaddlePaddle's flash attention with global sliding window support.
+    
+    Args:
+        q, k, v: Query, key, value tensors
+        window_size: Size of sliding window
+        num_global_tokens: Number of global tokens that receive attention from all positions
+        dropout_p: Dropout probability
+        softmax_scale: Scaling factor for softmax
+        causal: Whether to use causal attention
+        
+    Returns:
+        torch.Tensor: Output of flash attention
+    """
+    if not causal:
+        raise NotImplementedError("Non-causal sliding window attention with global tokens is not implemented here.")
+
+    B, H, T_q, D_q = q.shape
+    _, _, T_k, _ = k.shape
+    
+    # Create query and key indices 
+    q_idx = torch.arange(T_q, device=q.device).view(1, 1, T_q, 1)
+    kv_idx = torch.arange(T_k, device=k.device).view(1, 1, 1, T_k)
+
+    # Generate the attention mask - True means positions to attend to
+    attn_mask_bool = sliding_window_causal_idx(
+        b=B,  # Not used by sliding_window_causal_idx 
+        h=H,  # Not used by sliding_window_causal_idx
+        q_idx=q_idx, 
+        kv_idx=kv_idx, 
+        sliding_window=window_size, 
+        persistent_tokens=num_global_tokens
+    )
+
+    # Invert the mask: True for positions to mask out (not attend)
+    attn_mask_for_sdpa = ~attn_mask_bool
+    
+    # Use PyTorch's scaled_dot_product_attention with the custom mask
+    out = torch.nn.functional.scaled_dot_product_attention(
+        q, 
+        k, 
+        v, 
+        attn_mask=attn_mask_for_sdpa, 
+        dropout_p=dropout_p,
+        is_causal=False,  # Using custom mask instead
+        scale=softmax_scale  # Directly use softmax_scale with the scale parameter
+    )
+    
+    return out.contiguous()
+
 
 def dispatch_flash_attn(
     q: torch.Tensor,
diff --git a/src/olmo_core/nn/titans/neural_memory.py b/src/olmo_core/nn/titans/neural_memory.py
index 1fa4549..282619a 100644
--- a/src/olmo_core/nn/titans/neural_memory.py
+++ b/src/olmo_core/nn/titans/neural_memory.py
@@ -146,7 +146,10 @@ class NeuralMemory(nn.Module):
     2. but has shared, learned K and V matrices ad 
     """
 
-    def __init__(self, emb_dim = 16, n_layers = 2, hidden_dim = 32, alpha = 0.999, eta = 0.60, theta = 0.05, nu = 0.01):
+    def __init__(self, emb_dim = 16, n_layers = 2, 
+                 hidden_dim = 32, alpha = 0.999, 
+                 eta = 0.60, theta = 0.05, nu = 0.01,
+                 use_paddle_flash = False, num_global_tokens = 0):
         super().__init__()
 
         # Define the layers of the network
@@ -162,6 +165,14 @@ class NeuralMemory(nn.Module):
         self.K = nn.Linear(emb_dim, emb_dim, bias = False)  # Mapping to keys
         self.Q = nn.Linear(emb_dim, emb_dim, bias = False)  # Mapping to queries
         self.V = nn.Linear(emb_dim, emb_dim, bias = False)  # Mapping to values
+        
+        self.use_paddle_flash = use_paddle_flash
+        self.num_global_tokens = num_global_tokens
+        
+        self.persistent_tokens = nn.Parameter(
+        torch.empty(self.num_global_tokens, self.emb_dim)
+        )
+        torch.nn.init.normal_(self.persistent_tokens, mean=0.0, std=0.02)
 
         torch.nn.init.xavier_uniform_(self.K.weight)
         torch.nn.init.xavier_uniform_(self.V.weight)
@@ -229,7 +240,8 @@ class NeuralMemory(nn.Module):
     def forward(self, x):
         if self.mlps_processor is None or self.mlp_states[-1] is None:
             raise RuntimeError("MLPs not initialized. Call init_mlp(batch_size) first.")
-        queries = normalize(self.silu(self.Q(x)))
+        
+        queries = normalize(self.silu(self.Q(x)))            
         return functional_call(self.mlps_processor, self.mlp_states[-1], queries)
 
     @torch.compile(fullgraph=True)
@@ -238,7 +250,21 @@ class NeuralMemory(nn.Module):
             raise RuntimeError("MLPs not initialized. Call init_mlp(batch_size) first.")
         
         self.mlp_reset = False
+            
+            
         z = x.detach()
+           
+        # NOT SURE IF THIS SHOULD GO BEFORE OR AFTER THE DETATCH
+        
+        if self.use_paddle_flash:
+            # Add batch dimension [num_global_tokens, emb_dim] -> [1, num_global_tokens, emb_dim]
+            repeated_persistent_tokens = self.persistent_tokens.unsqueeze(0)
+            
+            # Expand to match batch size [1, num_global_tokens, emb_dim] -> [batch_size, num_global_tokens, emb_dim]
+            repeated_persistent_tokens = repeated_persistent_tokens.expand(z.shape[0], -1, -1)
+            
+            # Concatenate with input along sequence dimension
+            z = torch.cat([repeated_persistent_tokens, z], dim=1)
 
         # Evaluate the corresponding keys and values
         keys = normalize(self.silu(self.K(z)))
@@ -252,7 +278,7 @@ class NeuralMemory(nn.Module):
             self.mlp_states[-1], self.surprise, keys, values, beta_vec, eta_vec, theta_vec
         )
         if self.training:
-            self.mlp_states.append(next_mlp_params)
+            self.mlp_states[-1] = next_mlp_params
         return losses
 
     def init_mlp_template_weights(self, seed=42):
diff --git a/src/olmo_core/nn/transformer/block.py b/src/olmo_core/nn/transformer/block.py
index 9f67f57..2769816 100644
--- a/src/olmo_core/nn/transformer/block.py
+++ b/src/olmo_core/nn/transformer/block.py
@@ -218,10 +218,15 @@ class MAGReorderedNormTransformerBlock(TransformerBlock):
         d_model = kwargs.get("d_model", None)
         assert d_model is not None and isinstance(d_model, int) and d_model > 0, "d_model must be a kwarg of type int and > 0"
         self.chunk_size = self.memory_config.chunk_size
+        
+        self.use_paddle_flash = self.attention.use_paddle_flash
+        self.num_global_tokens = self.attention.num_global_tokens
         self.memory = NeuralMemory(
             emb_dim=d_model,
             n_layers=self.memory_config.n_layers,
             hidden_dim=self.memory_config.hidden_dim_multiple * d_model,
+            use_paddle_flash = self.use_paddle_flash,
+            num_global_tokens = self.num_global_tokens,
         )
         # self.persistent_memory = nn.Parameter(
         #     torch.randn(self.memory_config.persistent_mem_len, d_model)
@@ -262,9 +267,16 @@ class MAGReorderedNormTransformerBlock(TransformerBlock):
         # If the cache is already initialized, that means we only need to get the latest memory update
         else:
             last_gate = self.memory.retrieve(last_x)
-            self.gate_cache = torch.cat([self.gate_cache, last_gate], 1)
-                                         
-        attn = self.attention(x, **kwargs)
+            self.gate_cache = torch.cat([self.gate_cache, last_gate], dim=1)
+                    # Add batch dimension [num_global_tokens, emb_dim] -> [1, num_global_tokens, emb_dim]
+        repeated_persistent_tokens = self.memory.persistent_tokens.unsqueeze(0)
+        
+        # Expand to match batch size [1, num_global_tokens, emb_dim] -> [batch_size, num_global_tokens, emb_dim]
+        repeated_persistent_tokens = repeated_persistent_tokens.expand(x.shape[0], -1, -1)
+        
+        x_with_persistent = torch.cat([repeated_persistent_tokens, x], dim=1) # Add persistent tokens to the sliding window attention  
+                            
+        attn = self.attention(x_with_persistent, **kwargs)[:, self.num_global_tokens:, :] # Remove persistent tokens from attention output
         attn_with_mem = nn.Sigmoid()(self.gate_cache) * attn
 
         h = x + self.dropout(self.attention_norm(attn_with_mem))
@@ -290,7 +302,14 @@ class MAGReorderedNormTransformerBlock(TransformerBlock):
             self.memory.train_initial_mlp()
         gates = torch.cat(gates, dim=1)  # concatenate the gates for the whole seq
         
-        attn = self.attention(x, **kwargs)
+        repeated_persistent_tokens = self.memory.persistent_tokens.unsqueeze(0)
+        
+        # Expand to match batch size [1, num_global_tokens, emb_dim] -> [batch_size, num_global_tokens, emb_dim]
+        repeated_persistent_tokens = repeated_persistent_tokens.expand(x.shape[0], -1, -1)
+        
+        x_with_persistent = torch.cat([repeated_persistent_tokens, x], dim=1)      
+        
+        attn = self.attention(x_with_persistent, **kwargs)[:, self.num_global_tokens:, :] # Remove persistent tokens from attention output
         attn_with_mem = nn.Sigmoid()(gates) * attn  # MAG gate
 
         # basic attn normalization + feed forward
diff --git a/src/olmo_core/nn/transformer/config.py b/src/olmo_core/nn/transformer/config.py
index 9559e08..0c2714c 100644
--- a/src/olmo_core/nn/transformer/config.py
+++ b/src/olmo_core/nn/transformer/config.py
@@ -178,6 +178,9 @@ class TransformerBlockConfig(Config):
     Dropout probability.
     """
     memory_config: Optional[MemoryConfig] = None
+    """
+    Memory settings. No memory if None
+    """
 
     def build(
         self,
@@ -976,19 +979,37 @@ class TransformerConfig(Config):
             feed_forward_moe=feed_forward_moe,
             layer_norm=layer_norm,
             **transformer_block_kwargs,
-        )
-        
+        )   
+        block_override_configs = {}
         if block_overrides:
             # Make sure all blocks have required fields filled in using values from the default block
-            for layer_idx, block_config in block_overrides.items():
+            for layer_idx, block_config_list in block_overrides.items():
+                block_config = block_config_list[0]
                 if block_config.attention is None:
                     block_config.attention = block.attention
+                    use_paddle_flash = block_config_list[1]
+                    num_global_tokens = block_config_list[2]
+                    if use_paddle_flash and num_global_tokens:
+                        block_config.attention = AttentionConfig(
+                                        name=att_type,
+                                        n_heads=n_heads,
+                                        n_kv_heads=n_kv_heads,
+                                        bias=False,
+                                        rope=RoPEConfig(name=rope_type, theta=rope_theta, scaling=rope_scaling),
+                                        qk_norm=layer_norm if qk_norm else None,
+                                        use_flash=use_flash,
+                                        dtype=dtype,
+                                        use_paddle_flash=use_paddle_flash,
+                                        num_global_tokens=num_global_tokens,
+                                        **kwargs,
+                                    )
                 if block_config.feed_forward is None:
                     block_config.feed_forward = block.feed_forward
                 if block_config.layer_norm is None:
                     block_config.layer_norm = block.layer_norm
                 if block_config.feed_forward_moe is None:
                     block_config.feed_forward_moe = block.feed_forward_moe
+                block_override_configs[layer_idx] = block_config
 
         return cls(
             d_model=d_model,
@@ -997,7 +1018,7 @@ class TransformerConfig(Config):
             block=block,
             lm_head=LMHeadConfig(layer_norm=layer_norm, bias=False, dtype=dtype),
             dtype=dtype,
-            block_overrides=block_overrides,
+            block_overrides=block_override_configs
         )
 
     @classmethod
diff --git a/src/olmo_core/nn/transformer/memory_transformer.py b/src/olmo_core/nn/transformer/memory_transformer.py
index 614fe1a..4b37b1d 100644
--- a/src/olmo_core/nn/transformer/memory_transformer.py
+++ b/src/olmo_core/nn/transformer/memory_transformer.py
@@ -108,13 +108,13 @@ class MemoryTransformer(nn.Module):
         self.dtype = dtype
         self.num_persistent = num_persistent
         
-        self.persistent_token_embeddings = nn.Parameter(
-            torch.empty(self.num_persistent, self.d_model, dtype=self.dtype)
-        )
+        #self.persistent_token_embeddings = nn.Parameter(
+        #    torch.empty(self.num_persistent, self.d_model, dtype=self.dtype)
+        #)
         # Initialize the parameter weights.
         # You might want to use a specific method from self.init_method or a custom initialization.
         # For minimality, we use a similar approach to standard embedding initialization.
-        nn.init.normal_(self.persistent_token_embeddings, mean=0.0, std=init_std)
+        #nn.init.normal_(self.persistent_token_embeddings, mean=0.0, std=init_std)
 
         self.embeddings = nn.Embedding(vocab_size, d_model, dtype=dtype, device=init_device)
         self.blocks = nn.ModuleDict()
@@ -451,13 +451,13 @@ class MemoryTransformer(nn.Module):
         )
         
         sequence_embeds = self.embeddings(input_ids) if self.embeddings is not None else input_ids
-        batch_size = sequence_embeds.size(0)
-        expanded_persistent_embeds = self.persistent_token_embeddings.unsqueeze(0).expand(
-            batch_size, -1, -1
-        )
+        #batch_size = sequence_embeds.size(0)
+        #expanded_persistent_embeds = self.persistent_token_embeddings.unsqueeze(0).expand(
+        #    batch_size, -1, -1
+        #)
         # Get embeddings but pass-through for non-existent layers to allow easy
         # pipeline parallel configuration.
-        h = torch.cat([expanded_persistent_embeds, sequence_embeds], dim=1)
+        h = sequence_embeds
 
         # Run each block.
         for block in self.blocks.values():
@@ -466,27 +466,17 @@ class MemoryTransformer(nn.Module):
                 mark_dynamic(h, (0, 1), strict=False)
             h = block(h, **block_kwargs)
             
-            
-        final_labels: Optional[torch.Tensor] = None
-        if labels is not None:
-            prefix_labels = torch.full(
-                (batch_size, self.num_persistent),
-                ignore_index,  # Use the same ignore_index for persistent tokens
-                device=labels.device,
-                dtype=labels.dtype,
-            )
-            final_labels = torch.cat([prefix_labels, labels], dim=1)
 
         # Get final logits but again pass-through in case of pipeline parallelism.
         if self.lm_head is not None:
             if self.compile_enabled:
                 mark_dynamic(h, (0, 1), strict=False)
                 if labels is not None:
-                    mark_dynamic(final_labels, (0, 1), strict=False) # type:ignore
+                    mark_dynamic(labels, (0, 1), strict=False) # type:ignore
             # NOTE: When TP is active we can't pass 'labels=None' or the hook from 'PrepareModuleInput'
             # will throw an exception.
             if labels is not None:
-                lm_head_kwargs["labels"] = final_labels
+                lm_head_kwargs["labels"] = labels
             return self.lm_head(h, **lm_head_kwargs)
         else:
             return h
diff --git a/src/scripts/load_model_test.py b/src/scripts/load_model_test.py
index b234828..a62d598 100644
--- a/src/scripts/load_model_test.py
+++ b/src/scripts/load_model_test.py
@@ -55,13 +55,14 @@ Question: should kwargs for Neural Memory go through TransformerConfigBlockConfi
 
 USE_MAG = True
 USE_SW = True
-MAX_TOKENS = 128
+MAX_TOKENS = 256
 PROFILE_MEM = False
-NUM_PERSISTENT = 6
-TRAIN_MODEL = True
+NUM_PERSISTENT = None # None for no persistent tokens
+USE_PERSISTENT = (NUM_PERSISTENT is not None)
+TRAIN_MODEL = False
 
 # Layers that should use memory (e.g., only layers 0, 5, 10)
-MEMORY_LAYERS = [0, 1, 2, 3, 4] # Maximum number of memory layers I can have without crashing on 20gb 5/19
+MEMORY_LAYERS = [0, 1,] # Maximum number of memory layers I can have without crashing on 20gb 5/19
 
 if sys.platform == "darwin":  # if macos:
     USE_SW = False
@@ -80,22 +81,29 @@ if USE_MAG:
         kwargs["memory_config"] = memory_config
     else:
         # Apply only to specific layers
-        kwargs["block_name"] = TransformerBlockType.reordered_norm
+        kwargs["block_name"] = TransformerBlockType.mag_reordered_norm
         block_overrides = {}
         
         # Create block configs for specific memory layers
         for layer_idx in MEMORY_LAYERS:
-            block_overrides[layer_idx] = TransformerBlockConfig(
-                name=TransformerBlockType.mag_reordered_norm,
-                attention=None,  # Will be filled by the config system
-                layer_norm=None,  # Will be filled by the config system
-                feed_forward=None,  # Will be filled by the config system
-                memory_config=memory_config,
-            )
+            block_overrides[layer_idx] = [
+                TransformerBlockConfig(
+                    name=TransformerBlockType.mag_reordered_norm,
+                    attention=None,  # Will be filled by the config system
+                    layer_norm=None,  # Will be filled by the config system
+                    feed_forward=None,  # Will be filled by the config system
+                    memory_config=memory_config
+                ),
+                USE_PERSISTENT,  # Enable PaddlePaddle
+                NUM_PERSISTENT  # Use persistent tokens as global tokens
+            ]
         kwargs["block_overrides"] = block_overrides
 
 if USE_SW:
-    kwargs["sliding_window"] = SlidingWindowAttentionConfig(pattern=[True], window_size=memory_config.window_size)
+    kwargs["sliding_window"] = SlidingWindowAttentionConfig(
+            pattern=[True], 
+            window_size=memory_config.window_size,
+    )
     kwargs["use_flash"] = True
 
 tok_cfg = TokenizerConfig.dolma2()
@@ -163,6 +171,7 @@ def generate(model, text, max_tokens=MAX_TOKENS) -> Generator[torch.types.Number
             yield next_token.item()
             # ending generation if EOS token is reached
             if next_token.item() == tokenizer.eos_token_id:
+                print("[Got EOS token]")
                 break
 
 if not TRAIN_MODEL:
@@ -172,7 +181,8 @@ if not TRAIN_MODEL:
         for token in generate(model, sample_text, max_tokens=MAX_TOKENS):
             streamed_token = tokenizer.decode([token], skip_special_tokens=True)
             print(streamed_token, end="", flush=True)
-            if token == tokenizer.eos_token:
+            if token == tokenizer.eos_token_id:
+                print("[Got EOS token]")
                 break
         print("[Max Tokens Reached]")
     if PROFILE_MEM:
