2025-02-05 19:27:40.531	Dustin-Schwenk-MacBook-Pro:0	olmo_core.utils:195	INFO	Setting env var 'OMP_NUM_THREADS' to '8'
2025-02-05 19:27:40.531	Dustin-Schwenk-MacBook-Pro:0	olmo_core.utils:195	INFO	Setting env var 'TOKENIZERS_PARALLELISM' to 'false'
LcContTrain(
    run_name='lc_p1_8b_instruct_d1_r1',
    launch=BeakerLaunchConfig(
        name='lc_p1_8b_instruct_d1_r1-34c72d77',
        cmd=[
            'src/scripts/train/lc_cont_train/OLMo2-7B-lc_p1.py',
            'train',
            'lc_p1_8b_instruct_d1_r1',
            'ai2/jupiter-cirrascale-2',
            '--launch.num_nodes=8',
            '--launch.workspace=ai2-long-contexts'
        ],
        budget='ai2/oe-training',
        task_name='train',
        workspace='ai2-long-contexts',
        description=None,
        setup_steps=[
            'git clone "$REPO_URL" .',
            'git checkout "$GIT_REF"',
            'git submodule update --init --recursive',
            'conda shell.bash activate base',
            "pip install -e '.[all]'",
            'pip install --upgrade beaker-py',
            'pip freeze',
            'mkdir -p ~/.aws',
            'printenv AWS_CONFIG > ~/.aws/config',
            'printenv AWS_CREDENTIALS > ~/.aws/credentials'
        ],
        beaker_image='olmo-core-tch260dev20241209cu124',
        num_nodes=8,
        num_gpus=8,
        clusters=['ai2/jupiter-cirrascale-2'],
        shared_filesystem=True,
        priority=<Priority.normal: 'normal'>,
        preemptible=True,
        retries=None,
        env_vars=[BeakerEnvVar(name='NCCL_DEBUG', value='WARN')],
        env_secrets=[
            BeakerEnvSecret(name='BEAKER_TOKEN', secret='dustins_BEAKER_TOKEN'),
            BeakerEnvSecret(name='WANDB_API_KEY', secret='dustins_WANDB_API_KEY'),
            BeakerEnvSecret(name='COMET_API_KEY', secret='dustins_COMET_API_KEY'),
            BeakerEnvSecret(name='AWS_CONFIG', secret='dustins_AWS_CONFIG'),
            BeakerEnvSecret(name='AWS_CREDENTIALS', secret='dustins_AWS_CREDENTIALS'),
            BeakerEnvSecret(name='R2_ENDPOINT_URL', secret='R2_ENDPOINT_URL'),
            BeakerEnvSecret(name='WEKA_ENDPOINT_URL', secret='WEKA_ENDPOINT_URL'),
            BeakerEnvSecret(name='SLACK_WEBHOOK_URL', secret='SLACK_WEBHOOK_URL')
        ],
        nfs=False,
        weka_buckets=[BeakerWekaBucket(bucket='oe-training-default', mount='/weka/oe-training-default')],
        allow_dirty=False
    ),
    model=TransformerConfig(
        d_model=4096,
        vocab_size=100352,
        n_layers=32,
        block=TransformerBlockConfig(
            attention=AttentionConfig(
                name='default',
                n_heads=32,
                n_kv_heads=None,
                bias=False,
                rope=RoPEConfig(name='default', theta=658623, full_precision=True, scaling=None),
                clip_qkv=None,
                qk_norm=LayerNormConfig(name='rms', eps=1e-06, elementwise_affine=None, bias=False, full_precision=None, dtype='float32'),
                dropout=None,
                use_flash=False,
                dtype='float32'
            ),
            layer_norm=LayerNormConfig(name='rms', eps=1e-06, elementwise_affine=None, bias=False, full_precision=None, dtype='float32'),
            feed_forward=FeedForwardConfig(hidden_size=11008, name='default', bias=False, dtype='float32'),
            feed_forward_moe=None,
            name='reordered_norm',
            dropout=None
        ),
        lm_head=LMHeadConfig(
            name='default',
            layer_norm=LayerNormConfig(name='rms', eps=1e-06, elementwise_affine=None, bias=False, full_precision=None, dtype='float32'),
            bias=False,
            dtype='float32'
        ),
        name='default',
        dtype='float32',
        init_method='normal',
        init_seed=0,
        compile=True,
        dp_config=TransformerDataParallelConfig(name='hsdp', param_dtype='bfloat16', reduce_dtype='float32', num_replicas=None, wrapping_strategy='blocks'),
        tp_config=None,
        ac_config=TransformerActivationCheckpointingConfig(mode='selected_modules', block_interval=None, modules=['blocks.*.feed_forward']),
        float8_config=None
    ),
    optim=AdamWConfig(
        group_overrides=[OptimGroupOverride(params=['embeddings.weight'], opts={'weight_decay': 0.0})],
        compile=True,
        lr=2e-05,
        betas=[0.9, 0.95],
        eps=1e-08,
        weight_decay=0.1,
        foreach=None,
        fused=None
    ),
    dataset=NumpyDatasetConfig(
        tokenizer=TokenizerConfig(vocab_size=100278, eos_token_id=100257, pad_token_id=100277, bos_token_id=None, identifier='allenai/dolma2-tokenizer'),
        name='fsl',
        source_mixture_config=None,
        sequence_length=32768,
        max_target_sequence_length=None,
        max_sequence_length=None,
        min_sequence_length=None,
        vsl_curriculum=None,
        paths=None,
        mix='dolmino',
        mix_base_dir='/weka/oe-training-default/ai2-llm',
        dtype=None,
        metadata=None,
        include_instance_metadata=True,
        generate_doc_lengths=False,
        expand_glob=False,
        work_dir='/weka/oe-training-default/ai2-llm/checkpoints/dustins/dataset-cache'
    ),
    data_loader=NumpyDataLoaderConfig(global_batch_size=8388608, seed=34521, work_dir=None, num_threads=None, num_workers=4, prefetch_factor=None, target_device_type=None),
    trainer=TrainerConfig(
        save_folder='weka://oe-training-default/ai2-llm/checkpoints/dustins/lc_p1_8b_instruct_d1_r1',
        rank_microbatch_size=32768,
        work_dir=None,
        load_path='s3://ai2-llm/checkpoints/dustins/long-contexts/OLMo-2-1124-7B-Instruct/',
        load_strategy='if_available',
        load_key_mapping=None,
        checkpointer=CheckpointerConfig(work_dir=None, save_overwrite=None, pre_download=False, save_thread_count=1, load_thread_count=32, throttle_uploads=True),
        device=None,
        save_overwrite=True,
        max_duration=Duration(value=20000000, unit='tokens'),
        cancel_check_interval=10,
        hard_stop=None,
        metrics_collect_interval=10,
        callbacks={
            'checkpointer': CheckpointerCallback(
                save_interval=1000,
                ephemeral_save_interval=None,
                pre_train_checkpoint=None,
                save_async=True,
                remove='ephemeral_only',
                _latest_checkpoint_step=-1,
                _latest_checkpoint_path='',
                _checkpoints=[],
                _ephemeral_checkpoints=[],
                _checkpoints_to_remove=[]
            ),
            'wandb': WandBCallback(
                enabled=True,
                name='lc_p1_8b_instruct_d1_r1',
                project='long-contexts',
                entity='ai2-llm',
                group=None,
                tags=None,
                notes=None,
                config=None,
                cancel_tags=['cancel', 'canceled', 'cancelled'],
                cancel_check_interval=10
            ),
            'lr_scheduler': SchedulerCallback(scheduler=CosWithWarmup(lr_field='lr', initial_lr_field='initial_lr', warmup_steps=2000, alpha_f=0.1, t_max=None, warmup_min_lr=0.0)),
            'gpu_monitor': GPUMemoryMonitorCallback(device_id=None, _num_alloc_retries=0),
            'grad_clipper': GradClipperCallback(max_grad_norm=1.0),
            'config_saver': ConfigSaverCallback(config=None, fname='config.json'),
            'garbage_collector': GarbageCollectorCallback(gc_interval=1000, enabled=True, _start_state=None),
            'downstream_evaluator': DownstreamEvaluatorCallbackConfig(
                tasks=[
                    'mmlu_stem_mc_5shot',
                    'mmlu_humanities_mc_5shot',
                    'mmlu_social_sciences_mc_5shot',
                    'mmlu_other_mc_5shot',
                    'mmlu_stem_mc_5shot_test',
                    'mmlu_humanities_mc_5shot_test',
                    'mmlu_social_sciences_mc_5shot_test',
                    'mmlu_other_mc_5shot_test',
                    'arc_challenge_test_rc_5shot',
                    'arc_easy_test_rc_5shot',
                    'csqa_val_rc_5shot',
                    'hellaswag_val_rc_5shot',
                    'openbookqa_test_rc_5shot',
                    'piqa_val_rc_5shot',
                    'socialiqa_val_rc_5shot'
                ],
                tokenizer=TokenizerConfig(vocab_size=100278, eos_token_id=100257, pad_token_id=100277, bos_token_id=None, identifier='allenai/dolma2-tokenizer'),
                eval_batch_size=None,
                eval_interval=1000,
                eval_duration=Duration(value=1, unit='epochs'),
                log_interval=5,
                enabled=True
            )
        },
        fused_loss=True,
        compile_loss=False,
        z_loss_multiplier=1e-05,
        autocast_precision=None,
        async_bookkeeping=None
    ),
    load_path='s3://ai2-llm/checkpoints/dustins/long-contexts/OLMo-2-1124-7B-Instruct/'
)
